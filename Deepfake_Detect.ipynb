{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNr6JYRSEhhPNu+Amrc1kk/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thesushpatil/DeepFake_Video_Detection/blob/master/Deepfake_Detect.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 1: Setup and Imports\n",
        "# ==============================================================================\n",
        "# We'll import all the modules we need for data processing, model building,\n",
        "# and evaluation. All of these are pre-installed in Google Colab.\n",
        "# ==============================================================================\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "\n",
        "print(\"TensorFlow Version:\", tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3hi1kwwEiJz",
        "outputId": "ccfa33e4-c8a3-400c-b2c0-05ec0b53622e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow Version: 2.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 2: Configuration and Mounting Google Drive\n",
        "# ==============================================================================\n",
        "# This block contains all the key parameters for our experiment.\n",
        "# It also mounts your Google Drive to give Colab access to your dataset.\n",
        "# ❗ IMPORTANT ❗ -> You must update 'DATASET_BASE_PATH' to your folder path.\n",
        "# ==============================================================================\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Configuration\n",
        "class Config:\n",
        "    # ❗ UPDATE THIS PATH to your FaceForensics++ dataset in Google Drive\n",
        "    DATASET_BASE_PATH = '/content/drive/MyDrive/Datasets/forensics++/'\n",
        "\n",
        "    # We will use a subset of videos to make training faster for this example\n",
        "    MAX_VIDEOS_PER_CLASS = 150 # Max videos from 'real' and 'fake' folders\n",
        "    FRAMES_PER_VIDEO = 20      # Number of frames to extract from each video\n",
        "\n",
        "    # Model and Training Parameters\n",
        "    IMAGE_SIZE = (224, 224)\n",
        "    BATCH_SIZE = 32\n",
        "    LEARNING_RATE = 0.001\n",
        "    EPOCHS = 15\n",
        "\n",
        "# Please verify these paths match your folder structure.\n",
        "REAL_VIDEOS_PATH = os.path.join(Config.DATASET_BASE_PATH, '/content/drive/MyDrive/Datasets/forensics++/FF++/real/')\n",
        "FAKE_VIDEOS_PATH = os.path.join(Config.DATASET_BASE_PATH, '/content/drive/MyDrive/Datasets/forensics++/FF++/fake/')\n",
        "\n",
        "# Define the output directory for frames\n",
        "FRAME_OUTPUT_DIR = '/content/drive/MyDrive/Datasets/frames/'\n",
        "\n",
        "\n",
        "print(\"Verified the Path of the folders\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCbuuhg0E2P1",
        "outputId": "9f14f970-bcfd-4634-a93d-1d1adad9ed87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Verified the Path of the folders\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 3: Data Preparation - Frame Extraction\n",
        "# ==============================================================================\n",
        "# This cell scans the dataset folders and extracts a fixed number of frames\n",
        "# from each video, saving them to a temporary directory in Colab.\n",
        "# This process can take a few minutes.\n",
        "# ==============================================================================\n",
        "\n",
        "FRAME_OUTPUT_DIR = '/content/drive/MyDrive/Datasets/frames/'\n",
        "if not os.path.exists(FRAME_OUTPUT_DIR):\n",
        "    os.makedirs(os.path.join(FRAME_OUTPUT_DIR, 'real'))\n",
        "    os.makedirs(os.path.join(FRAME_OUTPUT_DIR, 'fake'))\n",
        "\n",
        "def extract_frames(video_path, output_folder, max_frames):\n",
        "    \"\"\"Extracts a fixed number of frames from a single video.\"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Could not open video {video_path}\")\n",
        "        return\n",
        "\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    if total_frames < max_frames:\n",
        "        return # Skip videos that are too short\n",
        "\n",
        "    frame_indices = np.linspace(0, total_frames - 1, max_frames, dtype=int)\n",
        "    video_filename = os.path.splitext(os.path.basename(video_path))[0]\n",
        "\n",
        "    count = 0\n",
        "    for i in frame_indices:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "        ret, frame = cap.read()\n",
        "        if ret:\n",
        "            frame_filename = f\"{video_filename}_frame_{count}.jpg\"\n",
        "            cv2.imwrite(os.path.join(output_folder, frame_filename), frame)\n",
        "            count += 1\n",
        "    cap.release()\n",
        "\n",
        "# Process Real Videos\n",
        "print(\"Extracting frames from REAL videos...\")\n",
        "real_videos = [os.path.join(REAL_VIDEOS_PATH, f) for f in os.listdir(REAL_VIDEOS_PATH)][:Config.MAX_VIDEOS_PER_CLASS]\n",
        "for video_file in real_videos:\n",
        "    extract_frames(video_file, os.path.join(FRAME_OUTPUT_DIR, 'real'), Config.FRAMES_PER_VIDEO)\n",
        "\n",
        "# Process Fake Videos\n",
        "print(\"Extracting frames from FAKE videos...\")\n",
        "fake_videos = [os.path.join(FAKE_VIDEOS_PATH, f) for f in os.listdir(FAKE_VIDEOS_PATH)][:Config.MAX_VIDEOS_PER_CLASS]\n",
        "for video_file in fake_videos:\n",
        "    extract_frames(video_file, os.path.join(FRAME_OUTPUT_DIR, 'fake'), Config.FRAMES_PER_VIDEO)\n",
        "\n",
        "print(\"Frame extraction complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRVOpEBfFtXE",
        "outputId": "0606be16-1d40-48af-9862-8cc21fcb228c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting frames from REAL videos...\n",
            "Extracting frames from FAKE videos...\n",
            "Frame extraction complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 3a: Face Detection and Cropping ✂️\n",
        "# ==============================================================================\n",
        "# This new cell processes every extracted frame. It detects the face in the\n",
        "# frame, crops it out, and overwrites the original frame with the cropped version.\n",
        "# This ensures our model only sees the face.\n",
        "# ==============================================================================\n",
        "import cv2\n",
        "import os\n",
        "from tqdm import tqdm # For a nice progress bar\n",
        "\n",
        "# Configuration - Added from a previous cell to ensure FRAME_OUTPUT_DIR is defined\n",
        "# Removed duplicate Config and FRAME_OUTPUT_DIR definition\n",
        "# class Config:\n",
        "#     # ❗ UPDATE THIS PATH to your FaceForensics++ dataset in Google Drive\n",
        "#     DATASET_BASE_PATH = '/content/drive/MyDrive/Datasets/forensics++/'\n",
        "\n",
        "#     # We will use a subset of videos to make training faster for this example\n",
        "#     MAX_VIDEOS_PER_CLASS = 150 # Max videos from 'real' and 'fake' folders\n",
        "#     FRAMES_PER_VIDEO = 20      # Number of frames to extract from each video\n",
        "\n",
        "#     # Model and Training Parameters\n",
        "#     IMAGE_SIZE = (224, 224)\n",
        "#     BATCH_SIZE = 32\n",
        "#     LEARNING_RATE = 0.001\n",
        "#     EPOCHS = 15\n",
        "\n",
        "# FRAME_OUTPUT_DIR = '/content/drive/MyDrive/Datasets/frames/'\n",
        "if not os.path.exists(FRAME_OUTPUT_DIR):\n",
        "    os.makedirs(os.path.join(FRAME_OUTPUT_DIR, 'real'))\n",
        "    os.makedirs(os.path.join(FRAME_OUTPUT_DIR, 'fake'))\n",
        "\n",
        "\n",
        "# Load OpenCV's Haar Cascade for face detection\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "def crop_face_from_frame(image_path):\n",
        "    \"\"\"Loads an image, detects the face, crops it, and saves it back using OpenCV.\"\"\"\n",
        "    # Read the image\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        print(f\"Warning: Could not read image {image_path}\")\n",
        "        return\n",
        "\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Detect faces in the image\n",
        "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
        "\n",
        "    if len(faces) > 0:\n",
        "        # Get the first detected face (most likely the primary subject)\n",
        "        (x, y, w, h) = faces[0]\n",
        "\n",
        "        # Add a small padding to the crop\n",
        "        padding = 20\n",
        "        x1 = max(0, x - padding)\n",
        "        y1 = max(0, y - padding)\n",
        "        x2 = min(image.shape[1], x + w + padding)\n",
        "        y2 = min(image.shape[0], y + h + padding)\n",
        "\n",
        "        # Crop the face\n",
        "        cropped_face = image[y1:y2, x1:x2]\n",
        "\n",
        "        # Save the cropped face back to the same file path\n",
        "        cv2.imwrite(image_path, cropped_face)\n",
        "        # print(f\"Cropped face saved to: {image_path}\")\n",
        "    # else:\n",
        "        # print(f\"No face detected in: {image_path}\")\n",
        "\n",
        "\n",
        "# Get a list of all frames to process\n",
        "real_frames_to_process = [os.path.join(FRAME_OUTPUT_DIR, 'real', f) for f in os.listdir(os.path.join(FRAME_OUTPUT_DIR, 'real'))]\n",
        "fake_frames_to_process = [os.path.join(FRAME_OUTPUT_DIR, 'fake', f) for f in os.listdir(os.path.join(FRAME_OUTPUT_DIR, 'fake'))]\n",
        "all_frames_to_process = real_frames_to_process + fake_frames_to_process\n",
        "\n",
        "# Run the cropping function on all frames with a progress bar\n",
        "print(\"Cropping faces from all extracted frames using OpenCV...\")\n",
        "for frame_path in tqdm(all_frames_to_process, desc=\"Processing frames\"):\n",
        "    crop_face_from_frame(frame_path)\n",
        "\n",
        "print(\"Face cropping complete.\")"
      ],
      "metadata": {
        "id": "Y7e455PQAev4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "outputId": "72fa11c2-7ca5-43ba-d1e8-33974d778e8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2358572038.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;31m# Get a list of all frames to process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0mreal_frames_to_process\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFRAME_OUTPUT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'real'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFRAME_OUTPUT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'real'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m \u001b[0mfake_frames_to_process\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFRAME_OUTPUT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fake'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFRAME_OUTPUT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fake'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0mall_frames_to_process\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal_frames_to_process\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfake_frames_to_process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 4: Create TensorFlow Data Pipeline (Modified for Error Handling)\n",
        "# ==============================================================================\n",
        "# This cell builds an efficient tf.data.Dataset pipeline. We've modified it\n",
        "# to handle potential errors during image loading and decoding.\n",
        "# We will now copy the data to local storage before creating the dataset.\n",
        "# ==============================================================================\n",
        "\n",
        "import shutil\n",
        "\n",
        "# Define the local storage path for frames\n",
        "LOCAL_FRAME_OUTPUT_DIR = '/content/frames/'\n",
        "\n",
        "# Ensure the local output directory exists\n",
        "if not os.path.exists(LOCAL_FRAME_OUTPUT_DIR):\n",
        "    os.makedirs(os.path.join(LOCAL_FRAME_OUTPUT_DIR, 'real'))\n",
        "    os.makedirs(os.path.join(LOCAL_FRAME_OUTPUT_DIR, 'fake'))\n",
        "\n",
        "# Copy frames from Google Drive to local storage\n",
        "print(f\"Copying frames from {FRAME_OUTPUT_DIR} to {LOCAL_FRAME_OUTPUT_DIR}...\")\n",
        "\n",
        "real_drive_frames = [os.path.join(FRAME_OUTPUT_DIR, 'real', f) for f in os.listdir(os.path.join(FRAME_OUTPUT_DIR, 'real'))]\n",
        "fake_drive_frames = [os.path.join(FRAME_OUTPUT_DIR, 'fake', f) for f in os.listdir(os.path.join(FRAME_OUTPUT_DIR, 'fake'))]\n",
        "\n",
        "for frame_path in real_drive_frames:\n",
        "    shutil.copy(frame_path, os.path.join(LOCAL_FRAME_OUTPUT_DIR, 'real', os.path.basename(frame_path)))\n",
        "\n",
        "for frame_path in fake_drive_frames:\n",
        "     shutil.copy(frame_path, os.path.join(LOCAL_FRAME_OUTPUT_DIR, 'fake', os.path.basename(frame_path)))\n",
        "\n",
        "print(\"Copying complete.\")\n",
        "\n",
        "\n",
        "# Now create the dataset using the local frame paths\n",
        "real_frame_paths = [os.path.join(LOCAL_FRAME_OUTPUT_DIR, 'real', f) for f in os.listdir(os.path.join(LOCAL_FRAME_OUTPUT_DIR, 'real'))]\n",
        "fake_frame_paths = [os.path.join(LOCAL_FRAME_OUTPUT_DIR, 'fake', f) for f in os.listdir(os.path.join(LOCAL_FRAME_OUTPUT_DIR, 'fake'))]\n",
        "all_frame_paths = real_frame_paths + fake_frame_paths\n",
        "labels = [0] * len(real_frame_paths) + [1] * len(fake_frame_paths)\n",
        "\n",
        "# Split data into training + test, and validation sets (80/20 split for validation)\n",
        "X_train_temp, X_val, y_train_temp, y_val = train_test_split(all_frame_paths, labels, test_size=0.2, random_state=42, stratify=labels)\n",
        "\n",
        "# Now split the remaining data (X_train_temp, y_train_temp) into training and test sets (e.g., 75/25 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train_temp, y_train_temp, test_size=0.25, random_state=42, stratify=y_train_temp) # 0.25 of 0.8 is 0.2\n",
        "\n",
        "print(f\"Total samples: {len(all_frame_paths)}\")\n",
        "print(f\"Training samples: {len(X_train)}\")\n",
        "print(f\"Validation samples: {len(X_val)}\")\n",
        "print(f\"Test samples: {len(X_test)}\")\n",
        "\n",
        "\n",
        "def parse_function(filename, label):\n",
        "    \"\"\"Reads an image file, decodes it, and applies preprocessing, handling errors.\"\"\"\n",
        "    image_string = tf.io.read_file(filename)\n",
        "\n",
        "    # Use tf.cond to handle empty or invalid image strings safely within the graph\n",
        "    def process_valid_image():\n",
        "        image = tf.image.decode_jpeg(image_string, channels=3)\n",
        "        image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "        image = tf.image.resize(image, Config.IMAGE_SIZE)\n",
        "        image = tf.keras.applications.efficientnet.preprocess_input(image)\n",
        "        return image, label\n",
        "\n",
        "    def handle_invalid_image():\n",
        "        # Return a placeholder tensor for invalid images.\n",
        "        # Use a shape and dtype consistent with the valid image processing path.\n",
        "        # A label of -1 will be used to filter these out later.\n",
        "        placeholder_image = tf.zeros(shape=(Config.IMAGE_SIZE[0], Config.IMAGE_SIZE[1], 3), dtype=tf.float32)\n",
        "        placeholder_label = tf.constant(-1, dtype=tf.int32) # Use tf.int32 for labels consistency\n",
        "        return placeholder_image, placeholder_label\n",
        "\n",
        "    # Check if the image string is valid. Decode_jpeg will also raise an error on invalid data,\n",
        "    # but checking length first can sometimes be faster for empty files.\n",
        "    # A more robust check might involve a try-except within tf.py_function,\n",
        "    # but tf.cond is generally preferred within tf.data.Dataset.map for performance.\n",
        "    is_valid = tf.greater(tf.strings.length(image_string), 0)\n",
        "\n",
        "    # Execute either process_valid_image or handle_invalid_image based on is_valid\n",
        "    return tf.cond(is_valid, process_valid_image, handle_invalid_image)\n",
        "\n",
        "\n",
        "def create_dataset(filepaths, labels):\n",
        "    \"\"\"Creates a tf.data.Dataset from filepaths and labels.\"\"\"\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((filepaths, labels))\n",
        "    dataset = dataset.map(parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    # Filter out invalid samples that were marked with label -1\n",
        "    dataset = dataset.filter(lambda x, y: tf.not_equal(y, -1))\n",
        "    dataset = dataset.shuffle(buffer_size=1024)\n",
        "    dataset = dataset.batch(Config.BATCH_SIZE)\n",
        "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "train_ds = create_dataset(X_train, y_train) # Add .repeat() here\n",
        "val_ds = create_dataset(X_val, y_val)\n",
        "# We don't create a test_ds pipeline in this cell, but you would typically do it here\n",
        "# test_ds = create_dataset(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "F3tusqfJAMqG",
        "outputId": "ccbfb42a-2f11-4f75-e71b-7dd0a73018fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying frames from /content/drive/MyDrive/Datasets/frames/ to /content/frames/...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1550181892.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mframe_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreal_drive_frames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLOCAL_FRAME_OUTPUT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'real'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mframe_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfake_drive_frames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m     \u001b[0mcopymode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    271\u001b[0m                     \u001b[0;32melif\u001b[0m \u001b[0m_USE_CP_SENDFILE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m                             \u001b[0m_fastcopy_sendfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfsrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m                             \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m                         \u001b[0;32mexcept\u001b[0m \u001b[0m_GiveupOnFastCopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36m_fastcopy_sendfile\u001b[0;34m(fsrc, fdst)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;31m# ...in oder to have a more informative exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 5: Build the Model\n",
        "# ==============================================================================\n",
        "# Here we define the model architecture using the pre-trained EfficientNetB0\n",
        "# as our base. This use of transfer learning is key to getting good results.\n",
        "# We will now make more layers of the base model trainable for potentially better accuracy.\n",
        "# ==============================================================================\n",
        "\n",
        "def build_model():\n",
        "    \"\"\"Builds the deepfake detection model using EfficientNetB0.\"\"\"\n",
        "    # We now use the standard EfficientNetB0\n",
        "    base_model = EfficientNetB0(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=(Config.IMAGE_SIZE[0], Config.IMAGE_SIZE[1], 3)\n",
        "    )\n",
        "\n",
        "    # Unfreeze the base model initially\n",
        "    base_model.trainable = True\n",
        "\n",
        "    # Freeze layers to control the number of trainable parameters.\n",
        "    # We are aiming for trainable parameters between 50,000 and 100,000.\n",
        "    # Based on previous attempts, freezing up to layer 235 gave a bit over 40k,\n",
        "    # and freezing up to 230 gave around 200k.\n",
        "    # Let's try freezing up to layer 232.\n",
        "    for layer in base_model.layers[:235]: # Adjust this number to target 50k-100k trainable params\n",
        "        layer.trainable = False\n",
        "\n",
        "\n",
        "    model = Sequential([\n",
        "        base_model,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(1, activation='sigmoid') # Sigmoid for binary classification\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "model = build_model()\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=Config.LEARNING_RATE),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()]\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "yUP4WpPZJtyy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "c1062768-7726-46c8-ec81-28fe1bd761c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ efficientnetb0 (\u001b[38;5;33mFunctional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m4,049,571\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │         \u001b[38;5;34m1,281\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ efficientnetb0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,281</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,050,852\u001b[0m (15.45 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,050,852</span> (15.45 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m413,441\u001b[0m (1.58 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">413,441</span> (1.58 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3,637,411\u001b[0m (13.88 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,637,411</span> (13.88 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 6: Train the Model (Corrected Save Path)\n",
        "# ==============================================================================\n",
        "# ❗ IMPORTANT ❗: We've changed the filepath to save directly to Google Drive.\n",
        "# Create a folder in your Drive for this project first.\n",
        "# ==============================================================================\n",
        "\n",
        "# Define a path in your Google Drive to save the model\n",
        "# Make sure you create this folder in your Google Drive!\n",
        "MODEL_SAVE_PATH = '/content/drive/MyDrive/Datasets/best_model.h5'\n",
        "\n",
        "\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
        "    filepath=MODEL_SAVE_PATH, # <-- This is the corrected path\n",
        "    save_best_only=True,\n",
        "    monitor=\"val_accuracy\"\n",
        ")\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(\n",
        "    patience=3, restore_best_weights=True, monitor=\"val_loss\"\n",
        ")\n",
        "\n",
        "print(\"\\nStarting model training...\")\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    epochs=Config.EPOCHS,\n",
        "    validation_data=val_ds,\n",
        "    callbacks=[checkpoint_cb, early_stopping_cb]\n",
        ")\n",
        "print(\"Model training complete. Best model saved to Google Drive.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtJexozqs8FE",
        "outputId": "23ea4a72-f633-4a34-b729-3f2318500ba2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting model training...\n",
            "Epoch 1/15\n",
            "    113/Unknown \u001b[1m42s\u001b[0m 154ms/step - accuracy: 0.4932 - loss: 0.7149 - precision_18: 0.4884 - recall_18: 0.4941"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/epoch_iterator.py:160: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 292ms/step - accuracy: 0.4932 - loss: 0.7149 - precision_18: 0.4884 - recall_18: 0.4943 - val_accuracy: 0.5000 - val_loss: 0.6937 - val_precision_18: 0.5000 - val_recall_18: 1.0000\n",
            "Epoch 2/15\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 98ms/step - accuracy: 0.5100 - loss: 0.7016 - precision_18: 0.5143 - recall_18: 0.4411 - val_accuracy: 0.5000 - val_loss: 0.7397 - val_precision_18: 0.5000 - val_recall_18: 1.0000\n",
            "Epoch 3/15\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 93ms/step - accuracy: 0.5081 - loss: 0.7011 - precision_18: 0.5110 - recall_18: 0.5525 - val_accuracy: 0.5000 - val_loss: 0.6949 - val_precision_18: 0.0000e+00 - val_recall_18: 0.0000e+00\n",
            "Epoch 4/15\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 89ms/step - accuracy: 0.5031 - loss: 0.7007 - precision_18: 0.4896 - recall_18: 0.4611 - val_accuracy: 0.5000 - val_loss: 0.6978 - val_precision_18: 0.5000 - val_recall_18: 1.0000\n",
            "Model training complete. Best model saved to Google Drive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 8: Evaluate the Model\n",
        "# ==============================================================================\n",
        "# Finally, we evaluate our trained model's performance on the validation set\n",
        "# and visualize the training history to check for issues like overfitting.\n",
        "# ==============================================================================\n",
        "\n",
        "# best_model = keras.models.load_model(\"best_model.h5\")\n",
        "\n",
        "# val_loss, val_acc, val_prec, val_recall = best_model.evaluate(val_ds)\n",
        "# print(f\"\\nValidation Accuracy: {val_acc:.4f}\")\n",
        "# print(f\"Validation Precision: {val_prec:.4f}\")\n",
        "# print(f\"Validation Recall: {val_recall:.4f}\")\n",
        "\n",
        "# def plot_history(history):\n",
        "#     \"\"\"Plots accuracy and loss curves for training and validation sets.\"\"\"\n",
        "#     fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "#     # Plot training & validation accuracy values\n",
        "#     ax1.plot(history.history['accuracy'])\n",
        "#     ax1.plot(history.history['val_accuracy'])\n",
        "#     ax1.set_title('Model Accuracy')\n",
        "#     ax1.set_ylabel('Accuracy')\n",
        "#     ax1.set_xlabel('Epoch')\n",
        "#     ax1.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "#     # Plot training & validation loss values\n",
        "#     ax2.plot(history.history['loss'])\n",
        "#     ax2.plot(history.history['val_loss'])\n",
        "#     ax2.set_title('Model Loss')\n",
        "#     ax2.set_ylabel('Loss')\n",
        "#     ax2.set_xlabel('Epoch')\n",
        "#     ax2.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "#     plt.show()\n",
        "\n",
        "# plot_history(history)"
      ],
      "metadata": {
        "id": "r8_xVWy3rqTu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}